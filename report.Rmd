```{r chunk1, echo=FALSE, results="hide", warnings="hide", message = FALSE, include=FALSE}

# CODE


library(caret)
library(rpart.plot)
library(randomForest)
set.seed(17092014)

rm(list = ls())

data_train_raw <- read.csv("C:/Users/jvosahlo/Videos/COURSERA/pml-training.csv", header = TRUE)
data_test <- read.csv("C:/Users/jvosahlo/Videos/COURSERA/pml-testing.csv", header = TRUE)

data_train_raw <- data_train_raw[,-1] # remove first order-predictor
data_train_raw <- data_train_raw[,-4] # remove cvtd_timestamp-predictor - not interpretable

# ------------------------------
# Preprocessing & transformation
#--------------------------------

# count blanks and NA values in each predictors

n_lin <- dim(data_train_raw)[1]
n_col <- dim(data_train_raw)[2]

blanks_count <- c(1:n_col)*0
na_count <- c(1:n_col)*0
for (i in 1:n_col) {
  for (j in 1:n_lin) {
    if (is.na(data_train_raw[j,i])) {
      na_count[i] = na_count[i]+1 }
    else if (data_train_raw[j,i] == "") {
      blanks_count[i] = blanks_count[i]+1}
  }
}

# leave out useless predictors - new_window parameter holds the dummy info
leave_out <- blanks_count+na_count < (n_lin/2)
data_train <- data_train_raw[,leave_out]
dim(data_train)[2]

# 57 predictors remain

# ------------------------------
# Measuring predictors importance
#--------------------------------

# Fitting a tree with 57 predictors
tree_model <- train(data_train$classe ~ ., method="rpart", data = data_train)
#rpart.plot(tree_model$finalModel)
#summary(tree_model)
prediction <- predict.train(tree_model, data_train)
accuracy <- sum(prediction==data_train$classe)/dim(data_train)[1]


zero_importance <- varImp(tree_model)$importance$Overall==0
important_vars <- row.names(varImp(tree_model)$importance)[(1-zero_importance) == 1]

important_index <- c(1:dim(data_train)[2])*0

for (i in 1:dim(data_train)[2]) {
  for (j in 1:length(important_vars)) {
    if (names(data_train)[i] == important_vars[j]) {important_index[i] <- 1}
  }
}

data_train_2 <- data_train[important_index==1]
data_train_2["classe"] <- NA
data_train_2$classe <- data_train$classe

names(data_train_2)
dim(data_train_2)
# 13 predictors remain

# ------------------------------
# k-fold cross validation - TREE (k = 10)
#--------------------------------

# create 10 disjunctive train/test set indices

test_sample_index <- matrix(0, nrow=10, ncol = round(dim(data_train_2)[1]/10))
train_sample_index <- matrix(0, nrow=10, ncol = dim(data_train_2)[1]-round(dim(data_train_2)[1]/10))

test_sample_index[1,] <-sample(1:dim(data_train_2)[1],size=dim(data_train_2)[1]/10,replace=F)
train_sample_index[1,] <- setdiff(1:dim(data_train_2)[1],test_sample_index[1,])

rest_of_index <- c(1:dim(data_train_2)[1])

for (i in 2:10) {
  rest_of_index <- setdiff(rest_of_index,test_sample_index[i-1,])
  test_sample_index[i,] <-sample(rest_of_index,size=dim(data_train_2)[1]/10,replace=F)
  train_sample_index[i,] <- setdiff(1:dim(data_train_2)[1],test_sample_index[i,])
}

# estimate 10 trees on each train set
accuracy <- c(1:10)*0

for (i in 1:10) {
  test_set <- data_train_2[test_sample_index[i,],]
  train_set <- data_train_2[train_sample_index[i,],]
  tree_model <- train(train_set$classe ~ ., method="rpart", data = train_set)
  
  prediction <- predict.train(tree_model, test_set)
  accuracy[i]<- sum(prediction==test_set$classe)/dim(test_set)[1]
}


#attributes(prediction)

#train = sample(1:dim(data_train_2)[1],size=dim(data_train_2)[1]/10,replace=F)


# ------------------------------
# Random forests
#--------------------------------

forest_model <- train(data_train_2$classe ~ ., method="rf", data = data_train_2)
#rpart.plot(forest_model$finalModel)

fitted_train_values <- predict.train(forest_model, data_train_2)
accuracy_forest <- sum(fitted_train_values==data_train_2$classe)/length(fitted_train_values)

#qplot(roll_forearm, classe, data = data_train)

# ------------------------------
# k-fold cross validation - RANDOM FOREST (k=3)
#--------------------------------

k=3
test_sample_index2 <- matrix(0, nrow=k, ncol = round(dim(data_train_2)[1]/k-1))
train_sample_index2 <- matrix(0, nrow=k, ncol = dim(data_train_2)[1]-round(dim(data_train_2)[1]/k-1))

test_sample_index2[1,] <-sample(1:dim(data_train_2)[1],size=dim(data_train_2)[1]/k,replace=F)
train_sample_index2[1,] <- setdiff(1:dim(data_train_2)[1],test_sample_index2[1,])

rest_of_index2 <- c(1:dim(data_train_2)[1])

for (i in 2:k) {
  rest_of_index2 <- setdiff(rest_of_index2,test_sample_index2[i-1,])
  test_sample_index2[i,] <-sample(rest_of_index2,size=dim(data_train_2)[1]/k,replace=F)
  train_sample_index2[i,] <- setdiff(1:dim(data_train_2)[1],test_sample_index2[i,])
}

# estimate 10 trees on each train set
accuracy2 <- c(1:k)*0

for (i in 1:k) {
  test_set2 <- data_train_2[test_sample_index2[i,],]
  train_set2 <- data_train_2[train_sample_index2[i,],]
  rforest_model <- train(train_set$classe ~ ., method="rpart", data = train_set)
  
  prediction2 <- predict.train(tree_model, test_set2)
  accuracy2[i]<- sum(prediction2==test_set2$classe)/dim(test_set2)[1]
}

# ------------------------------
# Prediction for testing sample
#--------------------------------

# forest_model <- train(data_train_2$classe ~ ., method="rf", data = data_train_2)

predicted_values <- predict.train(forest_model, data_test)

```














Preprocessing & transformation
========================================================
  
  Following adjustments were made on the raw training data to reduce number of predictors:
  - remove first (order) predictor
- remove cvtd_timestamp-predictor - suspiciously performing and not interpretable
- remove all predictors that are blanks/NA in more than 50% cases (these predictors are blanks/NA in the testing sample, so no information should be lost)

```
for (i in 1:n_col) {
  for (j in 1:n_lin) {
    if (is.na(data_train_raw[j,i])) {
      na_count[i] = na_count[i]+1 }
    else if (data_train_raw[j,i] == "") {
      blanks_count[i] = blanks_count[i]+1}
  }
}
leave_out <- blanks_count+na_count < (n_lin/2)
data_train <- data_train_raw[,leave_out]
```

After these adjustments 57 predictors remain.


Measuring predictors importance
========================================================
  
  On the remaining predictors random tree is estimated to assign importance to these variables.

```
tree_model <- train(data_train$classe ~ ., method="rpart", data = data_train)
```

Estimated tree on the whole training sample has folowing form:
  
  ```{r fig.width=7, fig.height=6}
rpart.plot(tree_model$finalModel)
```


```{r}
zero_importance <- varImp(tree_model)$importance$Overall==0
important_vars <- row.names(varImp(tree_model)$importance)[(1-zero_importance) == 1]
```


Predictors with Gini importance equal to zero are excluded from the model:
  
  Following 13 predictors remain:
  ```{r}
names(data_train_2)
```


k-fold cross validation - TREE (k = 10)
========================================================
  
  To estimate the tree model accuracy k-fold cross validation is applied with 10 random disjunctive testing samples from the original train sample.

```
test_sample_index[1,] <-sample(1:dim(data_train_2)[1],size=dim(data_train_2)[1]/10,replace=F)
train_sample_index[1,] <- setdiff(1:dim(data_train_2)[1],test_sample_index[1,])
rest_of_index <- c(1:dim(data_train_2)[1])

for (i in 2:10) {
  rest_of_index <- setdiff(rest_of_index,test_sample_index[i-1,])
  test_sample_index[i,] <-sample(rest_of_index,size=dim(data_train_2)[1]/10,replace=F)
  train_sample_index[i,] <- setdiff(1:dim(data_train_2)[1],test_sample_index[i,])
}
```
On each cross validation sample accuracy is estimated as 
```
accuracy[i]<- sum(prediction==test_set$classe)/dim(test_set)[1]

```
We receive following values:
  ```{r}
accuracy
```

Random forests
========================================================
  
  With 13 predictors, random forest can be applied with quite reasonable calculation time.

```
forest_model <- train(data_train_2$classe ~ ., method="rf", data = data_train_2)
```



k-fold cross validation - RANDOM FOREST (k=3)
========================================================
  
  For the Random forest k-fold cross validation, only 3 samples are taken due to larger time demand of the algorithm. The rest is the same as with the tree k-fold cross validation.

Results are considerably beter than the tree. 
```{r}
accuracy2 
```

Accuracy estimation is:
  ```{r}
mean(accuracy2)
```

Random forest is applied to the testing sample.
```{r}
summary(forest_model)
```
